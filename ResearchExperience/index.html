<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Research Experience · Shuo YANG</title><meta name="description" content="Research Experience - Shuo YANG"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Shuo YANG"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/About/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/CV/CV_ShuoYANG.pdf" target="_self" class="nav-list-link">CV</a></li><li class="nav-list-item"><a href="/ResearchExperience/" target="_self" class="nav-list-link">RESEARCH EXPERIENCE</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Research Experience</h1><div class="post-info">Dec 5, 2017</div><div class="post-content"><p>My recent projects with machine learning (especially reinfocement learning) is listed here, chronologically.</p>
<h4 id="Electrical-Market-Mechanism-Validation-with-Reinforcement-Learning"><a href="#Electrical-Market-Mechanism-Validation-with-Reinforcement-Learning" class="headerlink" title="Electrical Market Mechanism Validation with Reinforcement Learning"></a>Electrical Market Mechanism Validation with Reinforcement Learning</h4><p>This is my undergraduate thesis. It mainly focuses on the application of reinforcement learning to the electrical market. The ultimate goal of this project is optimizing the power plants’ biding policies simultaneously in a robust and efficient manner.</p>
<p>In this way, all the power plants can learn and present their optimal biding policy independently, the upper level (power plants) and the lower level (ISO) in the electrical market simulation will be decoupled. The extremely time-consuming bi-level optimization for the market equilibrium can then be reduced to a relatively simple market clearing process. Which create the possibility to validate different electrical market mechanism in large scale.</p>
<p>This project is in progress……</p>
<h4 id="Reinforcement-Learning-Fast-Planning-with-Linear-Dyna"><a href="#Reinforcement-Learning-Fast-Planning-with-Linear-Dyna" class="headerlink" title="Reinforcement Learning: Fast Planning with Linear Dyna"></a>Reinforcement Learning: Fast Planning with Linear Dyna</h4><p>In reinforcement learning problems, there is always a trade-off between data efficiency and computational efficiency. With the model-based method and function approximation, we can construct methods which can deal with a complex environment with high data efficiency. However, the model learning and planning can be quite computationally expensive.</p>
<p>This project focuses on improving the computational efficiency of <a href="https://arxiv.org/abs/1206.3285" target="_blank" rel="noopener">Linear Dyna</a>, a model-based RL algorithm with linear function approximation. By characterizing the parameters with the learned model explicitly with a set of linear equations, planning by sampling can then be simplified as solving the equations iteratively. This reduces the planning complexity from quadratic to linear.</p>
<p><a href="./source/fast-plan-linear.pdf">Here</a> is the draft of the work. I’m still testing other numerical methods for better robustness and efficiency. Most of this work was done during my summer intern in <a href="http://spaces.facsci.ualberta.ca/rlai/" target="_blank" rel="noopener">RLAI</a>.</p>
<h4 id="Reinforcement-Learning-with-External-Memory"><a href="#Reinforcement-Learning-with-External-Memory" class="headerlink" title="Reinforcement Learning with External Memory"></a>Reinforcement Learning with External Memory</h4><p>Much of reinforcement learning theory is based on the assumption of Markov property, the agent can then get all the information it needs from the current observation (State). This is however not a realistic in general. In dealing with the problem that has time dependency, it is desirable that the agent can identify and remember the relevant states.</p>
<p>This project focused on developing a RL algorithm with a limited-size memory module. I worked on the gradient estimation of the memory and also implemented the algorithm to empirical verify the idea. Experiments on multiple benchmarks showed that the new RL architecture was able to model the long-term dependencies.</p>
<p>Most of this work was done during my summer intern in <a href="http://spaces.facsci.ualberta.ca/rlai/" target="_blank" rel="noopener">RLAI</a>.</p>
<!-- [here](./projects/first_project.html) is the first project. -->
<h4 id="Automatic-Curriculum-Generation-in-Reinforcement-Learning"><a href="#Automatic-Curriculum-Generation-in-Reinforcement-Learning" class="headerlink" title="Automatic Curriculum Generation in Reinforcement Learning"></a>Automatic Curriculum Generation in Reinforcement Learning</h4><p>In curriculum learning, the agent will learn a serious of tasks, accumulating and transferring the knowledge they gained in those experience. It brings the ladder for the agent to conquer a difficult task. However, the curriculum learning faces a big limitation that the curriculum is largely relied on a human expert to design.</p>
<p>To address this problem, I integrated the <a href="https://www.cs.utexas.edu/~sanmit/papers/AAMAS16-Narvekar.pdf" target="_blank" rel="noopener">principles</a> to simplify the task with the automatic curriculum sequencing <a href="https://www.cs.utexas.edu/~sanmit/papers/IJCAI17-Narvekar.pdf" target="_blank" rel="noopener">method</a>. The target task will be broken into a series of simplified tasks, based on which the curriculum will be generated. Empirical results showed that this algorithm resulted in an increase in the speed of convergence by 40% and a greater improvement when the rewards are sparse.</p>
<p><a href="./source/automatic-curriculum-generation.pdf">Here</a> is the report of the experiments. Most of this work was done during my exchange semester at the University of Texas at Austin.</p>
</div></article></div></main><footer><div class="paginator"></div><div class="copyright"><p>© 2017 <a href="http://yoursite.com">Shuo YANG</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>